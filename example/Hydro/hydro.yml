#training
max_iters: 5000 #00 #180 #00
normalise_targets: True
normalise_margin: 0.10
test_frequency: 100 # 1000
ignore_nan: True
save_model: True
bassin: 0 # araguaiana 0 , barra 1 , caracarai 2 , conceicao 3, obidos 4 , bom 5 , all 6
p: 0.8
individual_loss: True # False
method: # None, linear_combination, mse


#Models
model:
  name: RFF #or RFF/SIREN/WIRES
  hidden_nlayers: 5
  hidden_width: 256
  scale: 5
  skip: False
  ## RFF specific
  mapping_size: 256
  activation: tanh
  modified_mlp: True #True # if you have a seperate encoder for the spatial and temporal inputs.
  linear: HE # RWF # HE # Glorot # RWF RWF does not work with SIREN
  # RWF Specific
  mean: 1
  std: 0.1

# Lambda values in the loss
validation_loss: mse
losses:
  mse:
    report: True
    bs: 16
    loss_balancing: True #wether to be included in loss balancing
    multiple_outputs: True
    ignore_nan: True
  pde:
    report: True
    log: False
    lambda: 1.e+2
    temporal_causality: True
    bs: 1024 # 8192 #16384 # 65536 #32768 # 8192 # √ # powers of 2 only
    method: pde
    loss_balancing: False #wether to be included in loss balancing
    penalty: L1
  # periodicity:
  #   report: False
  #   lambda: 1.e-1
  #   temporal_causality: True
  #   bs: 8192 # 8192 # 16384 # 32768 # powers of 2 only
  #   method: periodicity
  # spatial_grad:
  #   report: True
  #   log: True
  #   lambda: 1000000
  #   temporal_causality: True
  #   bs: 8192 # 8192 #16384 # 65536 #32768 # 8192 # √ # powers of 2 only
  #   method: spatial_gradient
  #   loss_balancing: False #wether to be included in loss balancing




# learning schemes
temporal_causality:
  M: 8 # powers of 2 only
  eps: 1.e+0
  step: 1 #update


# Loss balancing
relobralo:
  status: False
  T: 1.
  alpha: 0.999
  rho: 0.5
  step: 100

self_adapting_loss_balancing:
  status: True
  alpha: 0.90
  step: 100 #1000 #update


early_stopping:
  status: False
  ignore_first: 5
  patience: 20
  value: 0.0001

# optimizers
optimizer: AdamW
lr: 1.e-3
eps: 1.e-8 #adam precision
clip_gradients: True

learning_rate_decay:
  status: True
  step: 200
  gamma: 0.94 # 0.9

cosine_anealing:
  status: False
  min_eta: 0
  step: 500

optuna:
  patience: 10000
  trials: 100
